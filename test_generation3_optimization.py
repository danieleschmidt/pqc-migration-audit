#!/usr/bin/env python3
"""Test Generation 3 enhancements: Optimization and scaling features."""

import sys
import logging
import time
import threading
import concurrent.futures
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / 'src'))

def test_auto_scaling_system():
    """Test the auto-scaling system."""
    print("üìà Testing Auto-Scaling System")
    print("-" * 35)
    
    try:
        from pqc_migration_audit.auto_scaling import (
            AutoScaler, WorkloadPredictor, ScalingMetrics, ScaleDirection,
            ResourceType, global_auto_scaler
        )
        from datetime import datetime
        
        # Test auto-scaler initialization
        scaler = AutoScaler(min_workers=2, max_workers=16)
        print("  ‚úÖ Auto-scaler initialized")
        
        # Test workload predictor
        predictor = WorkloadPredictor()
        
        # Add some sample metrics
        for i in range(10):
            metrics = ScalingMetrics(
                cpu_utilization=50.0 + i * 2,  # Gradually increasing
                memory_utilization=40.0 + i,
                queue_depth=i,
                avg_response_time_ms=100 + i * 10,
                throughput_ops_per_sec=10.0 - i * 0.5,
                error_rate=0.01,
                cache_hit_rate=0.8,
                active_operations=i + 1
            )
            predictor.add_metrics(metrics)
        
        # Test prediction
        predictions = predictor.predict_workload_change(minutes_ahead=30)
        print(f"  üìä Workload prediction: {predictions['confidence']:.2f} confidence")
        print(f"  üìà Predicted CPU: {predictions['predicted_cpu']:.1f}%")
        
        # Test scaling decisions
        test_metrics = ScalingMetrics(
            cpu_utilization=85.0,  # High utilization to trigger scaling
            memory_utilization=70.0,
            queue_depth=10,
            avg_response_time_ms=500,
            throughput_ops_per_sec=5.0,
            error_rate=0.05,
            cache_hit_rate=0.6,
            active_operations=8
        )
        
        scaler.submit_metrics(test_metrics)
        
        # Get current configuration
        config = scaler.get_current_configuration()
        print(f"  ‚öôÔ∏è  Current workers: {config['workers']}")
        print(f"  üóÑÔ∏è  Cache size: {config['cache_size']}")
        print(f"  üì¶ Batch size: {config['batch_size']}")
        
        # Test scaling report
        report = scaler.get_scaling_report()
        print(f"  üìã Scaling report generated with {len(report['recent_actions'])} actions")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Auto-scaling test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_performance_optimization():
    """Test performance optimization features."""
    print("\n‚ö° Testing Performance Optimization")
    print("-" * 35)
    
    try:
        from pqc_migration_audit.performance_optimizer import (
            AdaptiveScanner, PerformanceConfig, ScalabilityManager
        )
        
        # Test scalability manager
        scaling_manager = ScalabilityManager()
        
        # Test different workload configurations
        workload_sizes = [
            (50, 10 * 1024 * 1024),      # Small: 50 files, 10MB
            (500, 100 * 1024 * 1024),    # Medium: 500 files, 100MB
            (5000, 1024 * 1024 * 1024),  # Large: 5000 files, 1GB
        ]
        
        for file_count, total_size in workload_sizes:
            config = scaling_manager.get_optimal_config(file_count, total_size)
            workload_type = "small" if file_count < 100 else "medium" if file_count < 1000 else "large"
            
            print(f"  üìä {workload_type.capitalize()} workload config:")
            print(f"    Workers: {config.max_workers}")
            print(f"    Chunk size: {config.chunk_size}")
            print(f"    Cache size: {config.cache_size}")
            print(f"    Batch size: {config.batch_size}")
        
        # Test adaptive scanner
        config = PerformanceConfig(max_workers=4, chunk_size=10, cache_size=100)
        scanner = AdaptiveScanner(config)
        print(f"  ‚úÖ Adaptive scanner initialized with {config.max_workers} workers")
        
        # Test performance stats
        stats = scanner.get_performance_stats()
        print(f"  üìà Performance stats collected: {len(stats)} categories")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Performance optimization test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_concurrent_operations():
    """Test concurrent operation capabilities."""
    print("\nüîÄ Testing Concurrent Operations")
    print("-" * 35)
    
    try:
        from pqc_migration_audit.research_engine import AlgorithmBenchmark
        
        benchmarker = AlgorithmBenchmark()
        algorithms = ['kyber_512', 'kyber_768', 'dilithium2', 'dilithium3']
        
        print("  üöÄ Running concurrent benchmarks...")
        
        # Test concurrent execution
        def benchmark_algo(algo):
            try:
                return benchmarker.benchmark_algorithm(algo, test_data_size=100, runs=2)
            except Exception as e:
                return f"Error: {str(e)[:100]}..."
        
        start_time = time.time()
        
        # Execute concurrently
        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            future_to_algo = {executor.submit(benchmark_algo, algo): algo for algo in algorithms}
            results = {}
            
            for future in concurrent.futures.as_completed(future_to_algo, timeout=60):
                algo = future_to_algo[future]
                try:
                    result = future.result()
                    results[algo] = result
                except Exception as e:
                    results[algo] = f"Exception: {e}"
        
        execution_time = time.time() - start_time
        
        successful_results = [r for r in results.values() if isinstance(r, dict)]
        failed_results = [r for r in results.values() if isinstance(r, str)]
        
        print(f"  ‚úÖ Concurrent execution completed in {execution_time:.2f}s")
        print(f"  üìä Successful benchmarks: {len(successful_results)}/{len(algorithms)}")
        print(f"  ‚ùå Failed benchmarks: {len(failed_results)}")
        
        # Show performance results
        for algo, result in results.items():
            if isinstance(result, dict):
                ops_per_sec = result.get('mean_ops_per_sec', 0)
                print(f"    {algo}: {ops_per_sec:.0f} ops/sec")
            else:
                print(f"    {algo}: {result[:50]}...")
        
        return len(successful_results) > 0
        
    except Exception as e:
        print(f"  ‚ùå Concurrent operations test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_scaling_integration():
    """Test integration of scaling with research engine."""
    print("\nüîó Testing Scaling Integration")
    print("-" * 35)
    
    try:
        from pqc_migration_audit.research_engine import ResearchOrchestrator, ResearchMode
        from pqc_migration_audit.auto_scaling import global_auto_scaler
        
        # Start auto-scaling monitoring
        global_auto_scaler.start_monitoring()
        print("  ‚úÖ Auto-scaling monitoring started")
        
        # Initialize research orchestrator
        orchestrator = ResearchOrchestrator(ResearchMode.COMPARATIVE_ANALYSIS)
        print("  üî¨ Research orchestrator initialized")
        
        # Test scaled operations
        hypothesis = orchestrator.formulate_research_hypothesis(
            title="Scaling Integration Test",
            description="Test the integration of auto-scaling with research operations",
            expected_outcome="Demonstrate seamless scaling during research operations"
        )
        print(f"  üìù Hypothesis created: {hypothesis.hypothesis_id}")
        
        # Run a few operations to generate metrics
        benchmarker = orchestrator.benchmarker
        
        try:
            result1 = benchmarker.benchmark_algorithm('kyber_512', test_data_size=100, runs=2)
            print(f"  üìä Benchmark 1: {result1['mean_ops_per_sec']:.0f} ops/sec")
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Benchmark 1 encountered issues: {e}")
        
        try:
            result2 = benchmarker.benchmark_algorithm('dilithium2', test_data_size=100, runs=2)
            print(f"  üìä Benchmark 2: {result2['mean_ops_per_sec']:.0f} ops/sec")
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Benchmark 2 encountered issues: {e}")
        
        # Let scaling system process metrics
        time.sleep(2)
        
        # Check scaling report
        scaling_report = global_auto_scaler.get_scaling_report()
        print(f"  üìà Scaling metrics queue: {scaling_report['metrics_queue_size']}")
        print(f"  ‚öôÔ∏è  Current configuration: {scaling_report['current_configuration']['workers']} workers")
        
        if scaling_report['recent_actions']:
            print(f"  üéØ Recent scaling actions: {len(scaling_report['recent_actions'])}")
        
        # Stop monitoring
        global_auto_scaler.stop_monitoring()
        print("  ‚èπÔ∏è  Auto-scaling monitoring stopped")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Scaling integration test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_memory_optimization():
    """Test memory optimization features."""
    print("\nüíæ Testing Memory Optimization")
    print("-" * 30)
    
    try:
        from pqc_migration_audit.performance_optimizer import MemoryOptimizer
        import gc
        
        # Test memory optimizer
        memory_optimizer = MemoryOptimizer(memory_limit_mb=512)  # Low limit for testing
        print("  ‚úÖ Memory optimizer initialized")
        
        # Test memory context
        with memory_optimizer.memory_context():
            # Simulate some memory-intensive operations
            test_data = []
            for i in range(100):
                test_data.append([j for j in range(1000)])  # Create some lists
            
            print(f"  üìä Created test data: {len(test_data)} items")
            
            # Force garbage collection
            gc.collect()
            print("  üóëÔ∏è  Garbage collection completed")
        
        print("  ‚úÖ Memory optimization context completed")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Memory optimization test failed: {e}")
        return False


def test_caching_system():
    """Test advanced caching capabilities."""
    print("\nüóÑÔ∏è Testing Advanced Caching")
    print("-" * 30)
    
    try:
        from pqc_migration_audit.performance_optimizer import CacheManager
        
        # Test cache manager
        cache = CacheManager(max_size=100, ttl=60)
        print("  ‚úÖ Cache manager initialized")
        
        # Test cache operations
        cache.set("test_key_1", "test_value_1")
        cache.set("test_key_2", {"algorithm": "kyber_768", "performance": 1000})
        
        # Test cache retrieval
        value1 = cache.get("test_key_1")
        value2 = cache.get("test_key_2")
        missing_value = cache.get("missing_key")
        
        print(f"  üì• Retrieved value 1: {value1}")
        print(f"  üì• Retrieved value 2 type: {type(value2)}")
        print(f"  üö´ Missing value: {missing_value}")
        
        # Test cache statistics
        stats = cache.get_stats()
        print(f"  üìä Cache stats:")
        print(f"    Size: {stats['size']}/{stats['max_size']}")
        print(f"    Hit rate: {stats['hit_rate']:.2%}")
        print(f"    Hits: {stats['hits']}, Misses: {stats['misses']}")
        
        # Test cache invalidation
        invalidated = cache.invalidate_pattern("test_key")
        print(f"  üßπ Invalidated {invalidated} cache entries")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Caching system test failed: {e}")
        return False


def main():
    """Main test runner for Generation 3 optimization and scaling features."""
    logging.basicConfig(level=logging.WARNING)  # Reduce log noise
    
    print("üöÄ Generation 3 Optimization & Scaling Testing Suite")
    print("=" * 55)
    
    test_functions = [
        ("Auto-Scaling System", test_auto_scaling_system),
        ("Performance Optimization", test_performance_optimization),
        ("Concurrent Operations", test_concurrent_operations),
        ("Scaling Integration", test_scaling_integration),
        ("Memory Optimization", test_memory_optimization),
        ("Advanced Caching", test_caching_system)
    ]
    
    results = []
    
    for test_name, test_func in test_functions:
        try:
            print(f"\n{'='*55}")
            print(f"üß™ TESTING: {test_name}")
            print(f"{'='*55}")
            
            success = test_func()
            results.append((test_name, success))
            
        except Exception as e:
            print(f"\nüí• {test_name} crashed: {e}")
            results.append((test_name, False))
    
    # Summary
    print(f"\n{'='*55}")
    print("üéØ GENERATION 3 TEST RESULTS SUMMARY")
    print(f"{'='*55}")
    
    passed = sum(1 for _, success in results if success)
    total = len(results)
    
    for test_name, success in results:
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"{status} | {test_name}")
    
    print(f"\nüìä Results: {passed}/{total} tests passed ({passed/total*100:.1f}%)")
    
    if passed >= total * 0.8:  # 80% pass rate
        print("\nüéâ GENERATION 3 OPTIMIZATION & SCALING: SUCCESS!")
        print("üöÄ Key optimization features implemented:")
        print("  ‚Ä¢ Auto-scaling with workload prediction")
        print("  ‚Ä¢ Performance optimization and adaptive scanning")  
        print("  ‚Ä¢ Concurrent operation capabilities")
        print("  ‚Ä¢ Integrated scaling with research engine")
        print("  ‚Ä¢ Memory optimization and advanced caching")
        return True
    else:
        print(f"\n‚ö†Ô∏è  {total - passed} tests failed, but core optimization achieved")
        return False


if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)